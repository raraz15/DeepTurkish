{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competent-sessions",
   "metadata": {},
   "source": [
    "# Corpus Former\n",
    "This notebook is forming a corpus from a Wiki Dump and the datasets that are used for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "registered-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../utilities')\n",
    "import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "following-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "[' ', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'y', 'z', 'ç', 'ö', 'ü', 'ğ', 'ı', 'ş']\n"
     ]
    }
   ],
   "source": [
    "# Read an alphabet\n",
    "dataset_name = \"METUbet\"\n",
    "dataset_dir = os.path.join(\"..\",\"data\",\"Datasets\",dataset_name,\"data\") # root directory of the dataset\n",
    "\n",
    "alphabet_dir = os.path.join(dataset_dir,'METUbet_alphabet.csv')\n",
    "alphabet_original = pd.read_csv(alphabet_dir,delimiter=\",\",header=None,encoding='utf8',skip_blank_lines=False)[0].tolist()\n",
    "alphabet_original.pop(0)\n",
    "print(\"--\"*40)\n",
    "print(alphabet_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedicated-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_work_directory = os.path.join(\"..\",\"data\",\"language model work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "happy-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the wiki dump text\n",
    "wiki_dump_path = os.path.join(lm_work_directory,\"wiki_00.txt\")\n",
    "with open(wiki_dump_path,'r', encoding='utf-8') as infile:\n",
    "    wiki_dump = infile.read()\n",
    "\n",
    "    \n",
    "# Find out where each article is\n",
    "iterator = re.finditer(r'<.+?>', wiki_dump)\n",
    "\n",
    "boundaries = []\n",
    "for i in iterator:    \n",
    "    boundaries.append((i.start(),i.end()))\n",
    "    \n",
    "    \n",
    "# Extract all the articles\n",
    "articles = []\n",
    "for i in range(len(boundaries)-1):\n",
    "    \n",
    "    article = wiki_dump[boundaries[i][1]:boundaries[i+1][0]]\n",
    "    \n",
    "    if len(article)>2: #skip 2 char strings\n",
    "        articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all articles\n",
    "clean_articles = [text_format.clean_text(article) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate cleaned articles into sentences\n",
    "clean_wiki_sentences = []\n",
    "for article in clean_articles:\n",
    "    \n",
    "    sentence_list = article.split(\". \")\n",
    "    \n",
    "    clean_wiki_sentences = text_format.filter_sentence_list(sentence_list, clean_wiki_sentences)\n",
    "    \n",
    "print(\"There are {} acceptable sentences.\".format(len(clean_wiki_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-circulation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counter = text_format.analyze_words(clean_wiki_sentences)\n",
    "\n",
    "bad_characters, bad_chars_str = text_format.analyze_symbols(clean_wiki_sentences,alphabet_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-tiffany",
   "metadata": {},
   "source": [
    "with open(\"bad_chars.txt\", 'w', encoding=\"utf8\") as f:\n",
    "    f.write(bad_chars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean from bad characters twice\n",
    "pattern1 = r\"\\s.+?[{}]+?.+?\\s\".format(bad_chars_str)\n",
    "pattern2 = r\"[{}].+?\\s\".format(bad_chars_str)\n",
    "\n",
    "super_diminished_sentences = []\n",
    "for sentence in clean_wiki_sentences:\n",
    "    \n",
    "    new_sentence = re.sub(pattern1, \" \", sentence)\n",
    "    new_sentence = re.sub(pattern2, \" \", new_sentence)\n",
    "    \n",
    "    super_diminished_sentences.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_characters2, bad_chars_str2 = text_format.analyze_symbols(super_diminished_sentences,alphabet_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take the sentences with turkish characters\n",
    "remaining_sentences = []\n",
    "#bad_sentences = []\n",
    "flag = False\n",
    "for sentence in super_diminished_sentences:\n",
    "    \n",
    "    for char in sentence:\n",
    "\n",
    "        if char not in alphabet_original:\n",
    "            flag = True\n",
    "            break\n",
    "            \n",
    "    if flag:\n",
    "        #bad_sentences.append(sentence)\n",
    "        flag = False\n",
    "        continue\n",
    "\n",
    "    if sentence and len(sentence) > 5:\n",
    "        remaining_sentences.append(sentence)\n",
    "        \n",
    "print(\"There are {} remaining sentences.\".format(len(remaining_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_word_dict = text_format.analyze_words(remaining_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exporting \n",
    "project_name = \"wiki_good_clean\"\n",
    "sent_list = remaining_sentences\n",
    "\n",
    "txt_path = os.path.join(lm_work_directory,\"{}.txt\".format(project_name))\n",
    "with open(txt_path, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in sent_list:\n",
    "        f.write(sentence+'\\n')\n",
    "\n",
    "json_path = os.path.join(lm_work_directory,\"{}-word_dict.json\".format(project_name))\n",
    "with open(json_path,'w', encoding='utf-8') as outfile:\n",
    "    json.dump(wiki_word_dict,outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-income",
   "metadata": {},
   "source": [
    "## Now read the sentences from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_datasets():\n",
    "\n",
    "    txt_dir = os.path.join(\"..\",'data',\"Datasets\",\"cv-corpus-5.1-2020-06-22\",\"tr\",\"cv-corpus-5.1-2020-06-22_validated_simple_sentences.txt\")\n",
    "    mozilla_sentences = text_format.read_txt(txt_dir,\"utf-8\")\n",
    "    print(\"Initially: {} sentences\".format(len(mozilla_sentences)))\n",
    "    \n",
    "    mozilla_sentences = list(filter(text_format.clean_text, mozilla_sentences))\n",
    "    \n",
    "    mozilla_sentences = text_format.filter_sentence_list(mozilla_sentences,clean_sentences=[],bound=0)\n",
    "    print(\"After Cleaning: {} sentences\".format(len(mozilla_sentences)))\n",
    "    \n",
    "    \n",
    "    txt_dir = os.path.join('..','data',\"Datasets\",\"METUbet\",\"data\",\"METUbet_sentences.txt\")\n",
    "    metu_sentences = text_format.read_txt(txt_dir,'utf-8')\n",
    "    print(\"\\nInitially: {} sentences\".format(len(metu_sentences)))\n",
    "    \n",
    "    metu_sentences = list(filter(text_format.clean_text, metu_sentences))\n",
    "\n",
    "    metu_sentences = text_format.filter_sentence_list(metu_sentences,clean_sentences=[],bound=0)\n",
    "    print(\"After Cleaning: {} sentences\".format(len(metu_sentences)))\n",
    "    \n",
    "    return mozilla_sentences, metu_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-press",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mozilla_sentences, metu_sentences = read_and_prepare_datasets() # read previous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exporting cleaned sentence lists\n",
    "txt_name = \"Mozilla_sentences.txt\"\n",
    "txt_path = os.path.join(lm_work_directory, txt_name)\n",
    "with open(txt_path, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in mozilla_sentences:\n",
    "        f.write(sentence+'\\n')\n",
    "        \n",
    "txt_name = \"METUbet_sentences.txt\"\n",
    "txt_path = os.path.join(lm_work_directory, txt_name)\n",
    "with open(txt_path, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in metu_sentences:\n",
    "        f.write(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-growing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN_datasets_sentences = text_format.merge_sentence_lists([mozilla_sentences, metu_sentences]) # merge the existing datasets\n",
    "\n",
    "NN_word_dict = text_format.analyze_words(NN_datasets_sentences) # investigate their word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the word dicts\n",
    "json_name = \"{}-word_dict.json\".format(\"NN_datasets\")\n",
    "with open(json_name,'w', encoding='utf-8') as outfile:\n",
    "    json.dump(NN_word_dict,outfile, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    \n",
    "text_name = \"{}-word_list.txt\".format(\"NN_datasets\")\n",
    "with open(text_name, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for word in NN_word_dict.keys():\n",
    "        f.write(word+'\\n')\n",
    "        \n",
    "# Export the merged sentences       \n",
    "txt_name = \"{}_sentences.txt\".format(\"NN_datasets\")\n",
    "with open(txt_name, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in NN_datasets_sentences:\n",
    "        f.write(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-saint",
   "metadata": {},
   "source": [
    "**Merge the Neural Network Training Sets and the cleaned Wikipedia DUmp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_dict = {**NN_word_dict, **wiki_word_dict}\n",
    "\n",
    "total_sentences = text_format.merge_sentence_lists([NN_datasets_sentences,remaining_sentences]) # merge all sentences\n",
    "\n",
    "filtered = text_format.filter_sentence_list(total_sentences,clean_sentences=[]) # clean and format each sentence\n",
    "print(\"There are {} sentences in the merged set.\".format(len(filtered)))\n",
    "\n",
    "bad_characters3, bad_chars_str3 = text_format.analyze_symbols(filtered,alphabet_original) # analyze characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-labor",
   "metadata": {},
   "source": [
    "**Dump the merged word dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for exporting the cleaned wiki dump\n",
    "txt_name = \"LM_sentences.txt\"\n",
    "with open(txt_name, 'w', encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in total_sentences:\n",
    "        f.write(sentence+'\\n')\n",
    "        \n",
    "        \n",
    "json_name = \"LM-word_dict.json\"\n",
    "with open(json_name,'w', encoding='utf-8') as outfile:\n",
    "    json.dump(total_word_dict,outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-charlotte",
   "metadata": {},
   "source": [
    "# For personal wiki scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_500 = read_txt(\"Wiki_sentences_500pages.txt\")\n",
    "print(\"{} sentences\".format(len(wiki_500)))\n",
    "\n",
    "wiki_500_cleaned = [clean_text(text) for text in wiki_500]\n",
    "\n",
    "wiki_500_filtered = filter_sentence_list(wiki_500_cleaned)\n",
    "print(\"{} sentences\".format(len(wiki_500_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = merge_wiki_scrapes([mozilla_sentences,metu_sentences,wiki500_cleaned,wiki_dump_sentences])\n",
    "print(len(total_sentences))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
