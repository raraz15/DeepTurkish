{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "#from turkish.deasciifier import Deasciifier !! not working\n",
    "import text_format\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Dataset Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose which dataset to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mozilla Turkish Dataset\n",
    "dataset_dir = os.path.join(\"..\",'data',\"Datasets\",\"cv-corpus-5.1-2020-06-22\",\"tr\")\n",
    "\n",
    "dataset_name = \"cv-corpus-5.1-2020-06-22\"+\"_validated\"\n",
    "\n",
    "new_dataset_name = dataset_name+'_simple'\n",
    "\n",
    "clip_dir = os.path.join(dataset_dir,\"clips\")\n",
    "tsv_dir = os.path.join(dataset_dir,\"validated.tsv\") # Original annotations\n",
    "\n",
    "df = pd.read_csv(tsv_dir,delimiter=\"\\t\")\n",
    "\n",
    "sentences_original = df['sentence'].tolist()\n",
    "paths = df['path'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#METUbet\n",
    "dataset_dir = os.path.join('..','data',\"Datasets\",\"METUbet\",\"data\")\n",
    "\n",
    "dataset_name = \"METUbet\"\n",
    "new_dataset_name = dataset_name\n",
    "\n",
    "#clip_dir = os.path.join(dataset_dir,\"clips\")\n",
    "csv_dir = os.path.join(dataset_dir,'METUbet.csv') # Original annotations\n",
    "\n",
    "df = pd.read_csv(csv_dir,sep=\",\")\n",
    "\n",
    "sentences_original = df['sentence'].tolist()\n",
    "IDs = df['path'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ti20 English Dataset\n",
    "tt = \"train\"\n",
    "\n",
    "dataset_name = \"ti20_\"+tt\n",
    "\n",
    "dataset_dir = os.path.join(\"..\",\"..\",\"Datasets\",\"ti20\",tt)\n",
    "\n",
    "csv_dir = os.path.join(dataset_dir,\"ti20_\"+tt+\".csv\")\n",
    "\n",
    "df = pd.read_csv(csv_dir,delimiter=\",\")\n",
    "\n",
    "sentences = df['sentence'].tolist()\n",
    "paths = df['path'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Format The Transcriptions\n",
    "\n",
    "Clean the transcriptions from non-Turkish characters first and format the the remaining transcriptions with posterior knowledge of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "true_alphabet = ['a','b','c','ç','d','e','f','g','ğ','h','ı','i','j','k','l','m','n','o','ö','p','r','s','ş','t','u','ü','v','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of sentences with unnecessary symbols. \n",
    "# Symbols found by posterior inspection\n",
    "non_Turkish = ['x','X','w','W','q','Q','ë']\n",
    "bad_indexes = []\n",
    "\n",
    "for idx,sentence in enumerate(sentences_original):    \n",
    "    for symbol in sentence:\n",
    "        \n",
    "        if symbol in non_Turkish:\n",
    "            bad_indexes.append(idx)\n",
    "            break\n",
    "\n",
    "print(\"{} utterences deleted from the data set.\".format(len(bad_indexes)))            \n",
    "\n",
    "df.drop(df.index[bad_indexes],inplace=True)\n",
    "df = df.reset_index(drop=True)            \n",
    "print(\"Dataset updated\")\n",
    "\n",
    "sentences_turkish = df['sentence'].tolist()\n",
    "paths = df['path'].tolist()\n",
    "print(\"Remaining sentences: {}.\".format(len(sentences_turkish)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mozilla**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the sentences\n",
    "sentences = [text_format.clean_text(sentence) for sentence in sentences_turkish if sentence] \n",
    "\n",
    "# Update dataframe\n",
    "df['sentence'] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METUbet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = text_format.METUbet_formatter(sentences_original) # metubet has de-ascifier indicators\n",
    "sentences = [text_format.clean_text(sentence) for sentence in sentences if sentence]\n",
    "\n",
    "df['turkish_sentence'] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.1)Sentence length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_counter = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_counter[sentence] += 1\n",
    "\n",
    "print(\"There are {} unique sentences.\".format(len(sentence_counter)))\n",
    "\n",
    "sentence_list = list(sentence_counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_name = new_dataset_name+\"_sentences.txt\"\n",
    "out_dir = os.path.join(dataset_dir,txt_name)\n",
    "\n",
    "with open(out_dir, 'w',encoding=\"utf8\") as f:\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        f.write(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.2)Word Count Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for sentence in sentences: #de_ascii_sentences:\n",
    "    for word in sentence.strip('.').split(' '):\n",
    "        \n",
    "        word_counter[word] += 1  \n",
    "    \n",
    "print(\"There are {} words in the dataset.\".format(len(word_counter)))\n",
    "\n",
    "print(\"\\nThe 10 most common words with the number of appearances:\\n\")\n",
    "n_most_common = word_counter.most_common(10)\n",
    "for pair in n_most_common:\n",
    "    print(\"{}\\t |\\t {}\".format(pair[0],pair[1]))\n",
    "    \n",
    "top_common = dict(word_counter.most_common(50))\n",
    "sorted_top_common = dict( sorted(top_common.items(), key=lambda x: x[0].lower()) )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(sorted_top_common.keys(), sorted_top_common.values())\n",
    "ax.set_ylabel('Number of Appearances',fontsize=14)\n",
    "ax.set_xlabel('Words',fontsize=14)\n",
    "ax.set_title('Top 100 Appearing Words',fontsize=14)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=9)\n",
    "plt.xticks(rotation=90)\n",
    "#plt.savefig(\"Sentece Length Distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Word Dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = dataset_name+\"-word_dict.json\"\n",
    "\n",
    "json_path = os.path.join(dataset_dir,json_name)\n",
    "with open(json_path,'w', encoding='utf-8') as outfile:\n",
    "    json.dump(word_counter,outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Word Dicts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = \"METUbet-word_dict.json\"\n",
    "with open(json_name,'r', encoding='utf-16') as infile:\n",
    "    word_counter2 = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set1 = set(word_counter)\n",
    "word_set2 = set(word_counter2)\n",
    "\n",
    "intersection = word_set1.intersection(word_set2)\n",
    "union = word_set1.union(word_set2)\n",
    "\n",
    "print(\"There are:\\n\")\n",
    "print(\"\\t{} words in set 1.\".format(len(word_set1)))\n",
    "print(\"\\t{} words in set 2.\".format(len(word_set2)))\n",
    "print(\"\\t{} words the intersection set.\".format(len(intersection)))\n",
    "print(\"\\t{} words in the union set.\".format(len(union)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERTurk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTurk_list = []\n",
    "with open(\"vocab_32k.txt\",encoding='utf-8') as fp:\n",
    "    line = fp.readline()\n",
    "    BERTurk_list.append(line.strip(\"\\n\"))\n",
    "    while line:\n",
    "        \n",
    "        line = fp.readline()\n",
    "        BERTurk_list.append(line.strip(\"\\n\"))\n",
    "\n",
    "BERTURK = BERTurk_list[1971:] # 1971 found by inspecting the text file    32k    \n",
    "#BERTURK = BERTurk_list[1925:] # 1925 found by inspecting the text file 128k\n",
    "\n",
    "suffix_list = [element for element in BERTURK if '#' in element]\n",
    "word_list = [element for element in BERTURK if '#' not in element]\n",
    "\n",
    "word_set_berturk = set(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigate_set = union\n",
    "\n",
    "common_words = word_set_berturk.intersection(investigate_set)\n",
    "print(\"There are {} common words ({:.2f}%).\".format(len(common_words),100*len(common_words)/len(investigate_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.3) Utterance Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_dict = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    length_dict[len(sentence)] += 1\n",
    "           \n",
    "sorted_items = sorted(length_dict.keys())\n",
    "mean = np.mean(sorted_items)\n",
    "var = np.var(sorted_items,dtype=np.float64)\n",
    "dev = np.std(sorted_items,dtype=np.float64)\n",
    "print(\"There are {} utterances.\\n\".format(len(sentences)))\n",
    "print(\"Mean of the utterance lengths {}\".format(mean))\n",
    "print(\"Variance: {}\".format(var))\n",
    "print(\"Standard Deviation: {:.1f}\\n\".format(dev))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(length_dict.keys(), length_dict.values())\n",
    "ax.set_ylabel('Number of Senteces',fontsize=14)\n",
    "ax.set_xlabel('Sentence Length',fontsize=14)\n",
    "ax.set_title('Sentence Length Distribution',fontsize=14)\n",
    "ax.set_xlim([0,max(sorted_items)+1])\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "#plt.savefig(\"Sentece Length Distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.4) Character Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_counter = Counter()\n",
    "\n",
    "for sentence in sentences:\n",
    "    for symbol in sentence:              \n",
    "        \n",
    "        symbol_counter[symbol] += 1 \n",
    "        \n",
    "print(\"Including the blank symbol(0), there are {} symbols.\".format(len(symbol_counter)+1))   \n",
    "\n",
    "alphabet = np.array(sorted(symbol_counter.keys(), key=lambda x:x.lower()))\n",
    "alphabet = np.insert(alphabet,0,'0') # The blank is added here!!!!\n",
    "print(\"\\nOur Alphabet:\")\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Alphabet\n",
    "\n",
    "use utf16 for Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = os.path.join(dataset_dir,new_dataset_name+'_alphabet.csv')\n",
    "pd.DataFrame(alphabet).to_csv(export_dir,index=False,header=False,encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D) Encode the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the alphabet, if the blank is included, correct the class ids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_encoded = []\n",
    "sentence_length = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    \n",
    "    encoded_sentence = []\n",
    "    \n",
    "    for symbol in sentence:\n",
    "        \n",
    "        class_id = np.where(alphabet==symbol)[0][0] # blank is included in the alphabet\n",
    "        encoded_sentence.append(int(class_id))\n",
    "        \n",
    "    sentences_encoded.append(encoded_sentence)\n",
    "    sentence_length.append(len(encoded_sentence))\n",
    "\n",
    "df['encoded'] = sentences_encoded\n",
    "df['sentence_length'] = sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E) Training and Test Set Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8 # train/total\n",
    "N_batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Total Samples: {}\\n\".format(df.shape[0]))\n",
    "N_train = int(df.shape[0]*split_ratio)\n",
    "N_train = N_train-(N_train%N_batch)\n",
    "\n",
    "#gets a random 80% of the entire set\n",
    "df_train = df.sample(n=N_train, random_state=1)\n",
    "#gets the left out portion of the dataset\n",
    "df_test = df.loc[~df.index.isin(df_train.index)].copy()\n",
    "\n",
    "print(\"Number of Training Samples: {}\".format(len(df_train)))\n",
    "N_test = df.shape[0]-N_train\n",
    "print(\"Number of Test Samples: {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E.1) Investigate the Test or Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a Subset and investigate\n",
    "sentences_invest = df_train['sentence'].tolist()\n",
    "sentences_invest = df_test['sentence'].tolist()\n",
    "\n",
    "length_dict = Counter()\n",
    "for sentence in sentences_invest :\n",
    "\n",
    "    length = len(sentence)\n",
    "    \n",
    "    length_dict[length] += 1\n",
    "        \n",
    "sorted_items = sorted(length_dict.keys())\n",
    "mean = np.mean(sorted_items)\n",
    "var = np.var(sorted_items,dtype=np.float64)\n",
    "dev = np.std(sorted_items,dtype=np.float64)\n",
    "print(\"There are {} utterances.\\n\".format(len(sentences_invest)))\n",
    "print(\"Mean of the utterance lengths {}\".format(mean))\n",
    "print(\"Variance: {}\".format(var))\n",
    "print(\"Standard Deviation: {:.2f}\\n\".format(dev))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(length_dict.keys(), length_dict.values())\n",
    "ax.set_ylabel('Number of Senteces',fontsize=14)\n",
    "ax.set_xlabel('Sentence Length',fontsize=14)\n",
    "ax.set_title('Sentence Length Distribution of the Training Set',fontsize=14)\n",
    "ax.set_xlim([0,max(sorted_items)+1])\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "#plt.savefig(\"Sentece Length Distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E.2) Create Specific Subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.2.1) Choose around mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_length_dict = dict()\n",
    "\n",
    "outside_dict = dict()\n",
    "\n",
    "total = 0\n",
    "for item in length_dict.items():\n",
    "    \n",
    "    if item[0] <= mean+dev and item[0] >= mean-dev:\n",
    "    \n",
    "        shorter_length_dict[item[0]] = item[1]\n",
    "        total += item[1]\n",
    "    else:\n",
    "        outside_dict[item[0]] = item[1]\n",
    "\n",
    "print(\"There are {} utterances around 1 standard deviation.\\n\".format(total))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(shorter_length_dict.keys(), shorter_length_dict.values())\n",
    "ax.bar(outside_dict.keys(), outside_dict.values(),color=\"Red\")\n",
    "\n",
    "#plt.text(60, 750, 'Utterences in the Dataset(Blue+Red): {}\\nMean of Utterance Lengths: {}\\nStandard Deviation: {:.1f}\\nUtterences in the Subset(Blue): {}'\\\n",
    "#         .format(len(sentences),mean,dev,total), fontsize=15,bbox=dict(alpha=0.5))\n",
    "\n",
    "ax.set_ylabel('Number of Utterances',fontsize=15)\n",
    "ax.set_xlabel('Utterance Length',fontsize=15)\n",
    "ax.set_title('Utterance Length Distribution',fontsize=15)\n",
    "ax.set_xlim([0,max(sorted_items)+1])\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.savefig(\"Utterance Length Distribution Subset.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.2.2)  Choose Specific Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_length_dict = dict()\n",
    "\n",
    "outside_dict = dict()\n",
    "\n",
    "total = 0\n",
    "for item in length_dict.items():\n",
    "    \n",
    "    if item[0] == 4:   \n",
    "        shorter_length_dict[item[0]] = item[1]\n",
    "        total += item[1]\n",
    "    else:\n",
    "        outside_dict[item[0]] = item[1]\n",
    "\n",
    "print(\"There are {} utterances with given length.\\n\".format(total))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "ax.bar(shorter_length_dict.keys(), shorter_length_dict.values())\n",
    "ax.bar(outside_dict.keys(), outside_dict.values(),color=\"Red\")\n",
    "\n",
    "ax.set_ylabel('Number of Utterances',fontsize=15)\n",
    "ax.set_xlabel('Utterance Length',fontsize=15)\n",
    "ax.set_title('Utterance Length Distribution',fontsize=15)\n",
    "ax.set_xlim([0,max(sorted_items)+1])\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.savefig(\"Utterance Length Distribution Subset.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_indices = []\n",
    "for idx,sentence in enumerate(sentences):\n",
    "    length = len(sentence)\n",
    "    \n",
    "    #if length >= mean+dev or length <= mean-dev:\n",
    "    if length != 4:\n",
    "        remove_indices.append(idx)\n",
    "                \n",
    "df.drop(df.index[remove_indices],inplace=True)\n",
    "df = df.reset_index(drop=True)            \n",
    "print(\"Subset Selected\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Dataset Based on Utterance Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['sentence_length'], ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True) # reset the index and drop it\n",
    "\n",
    "df_train.sort_values(by=['sentence_length'], ascending=True, inplace=True)\n",
    "df_train.reset_index(drop=False, inplace=True) # keep original indices in any case\n",
    "df_test.sort_values(by=['sentence_length'], ascending=True, inplace=True)\n",
    "df_test.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export new Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use pickle to preserve arrays, csv turns them into strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Türkçe\n",
    "new_name = new_dataset_name+\"_ordered\"\n",
    "\n",
    "df.to_pickle(os.path.join(dataset_dir,new_name+\".pkl\")) \n",
    "df.to_csv(os.path.join(dataset_dir,new_name+\".csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_name = new_name+'_train'\n",
    "\n",
    "df_train.to_pickle(os.path.join(dataset_dir,train_set_name+\".pkl\")) \n",
    "df_train.to_csv(os.path.join(dataset_dir,train_set_name+\".csv\"),index=False)\n",
    "\n",
    "test_set_name = new_name+'_test'\n",
    "\n",
    "df_test.to_pickle(os.path.join(dataset_dir,test_set_name+\".pkl\")) \n",
    "df_test.to_csv(os.path.join(dataset_dir,test_set_name+\".csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ti20\n",
    "new_dataset_name = dataset_name+'_coded'\n",
    "\n",
    "export_dir = os.path.join(\"..\",\"..\",\"Datasets\",\"ti20\",tt)\n",
    "\n",
    "df.to_pickle(os.path.join(export_dir,new_dataset_name+'.pkl')) \n",
    "df.to_csv(os.path.join(export_dir,new_dataset_name+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.to_csv(os.path.join(dataset_dir,\"validated_cleaned2.csv\"),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
