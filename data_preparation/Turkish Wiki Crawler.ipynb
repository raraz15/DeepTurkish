{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "known-employee",
   "metadata": {},
   "source": [
    "This notebook was intended to create a Wiki Crawler for Turkish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposed-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "norwegian-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url(url):\n",
    "    \n",
    "    response = requests.get(url=url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def scrapeWikiArticle(url):\n",
    "         \n",
    "    # OPEN URL\n",
    "    soup = open_url(url)\n",
    "    title = soup.find(id=\"firstHeading\").text\n",
    "    print(title)\n",
    "    \n",
    "    paragraph_texts = get_paragraphs(soup) # get the paragraph from the wiki\n",
    "    \n",
    "    \n",
    "    good_list = find_turkish_wikis(soup,3) # CREATE URL LIST\n",
    "        \n",
    "    return good_list, paragraph_texts, title\n",
    "\n",
    "\n",
    "#turkish_char = ['ç',\"Ç\",'ğ',\"Ğ\",'ı','ö',\"Ö\",'ş',\"Ş\",'ü',\"Ü\"] #,\"I\"\n",
    "\n",
    "def find_turkish_wikis(soup,k):\n",
    "    \"\"\"\n",
    "    Returns at most k turkish wiki link objects in a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\") # find new wikis\n",
    "    random.shuffle(allLinks)\n",
    "    \n",
    "    good_list = []\n",
    "    \n",
    "    for link in allLinks:\n",
    "        \n",
    "        try: \n",
    "            \n",
    "            if link['href'].find(\"/wiki/\") == -1: # only links to other wikis\n",
    "                continue\n",
    "                \n",
    "            link_title = link['title'] # find links with a title\n",
    "            \n",
    "            if (\":\" in link_title) or ('.' in link_title):  # don't include such pages                   \n",
    "                continue \n",
    "             \n",
    "            # check if the title includes a Turkish Character\n",
    "            for char in ['ç',\"Ç\",'ğ',\"Ğ\",'ı','ö',\"Ö\",'ş',\"Ş\",'ü',\"Ü\"]: #,\"I\"\n",
    "\n",
    "                if char in link_title: # Check Turkish characters\n",
    "                    good_list.append(link)\n",
    "                    break\n",
    "            \n",
    "            if len(good_list) == k:\n",
    "                break\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "                 \n",
    "    return good_list\n",
    "\n",
    "def get_paragraphs(soup):\n",
    "    \n",
    "    paragraphs = soup.find(id=\"bodyContent\").find_all('p') # find all paragraphs\n",
    "\n",
    "    paragraph_texts = []\n",
    "    for p in paragraphs:\n",
    "        cleaned_text = p.text.strip(\"\\n\")\n",
    "        if cleaned_text: # if the paragraph is not empty\n",
    "            paragraph_texts.append(cleaned_text)\n",
    "            \n",
    "    return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "renewable-amazon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Puma (şirket)\n",
      "Kamerun millî futbol takımı\n",
      "Cibuti millî futbol takımı\n",
      "Etiyopya millî futbol takımı\n",
      "Cezayir millî futbol takımı\n",
      "Fas millî futbol takımı\n",
      "Irak millî futbol takımı\n",
      "AFC Asya Kupası\n",
      "Uzatma süresi\n",
      "Endirekt serbest vuruş\n",
      "Taç atışı\n",
      "Topun oyunda ve oyun dışı olması\n",
      "Hakem atışı\n",
      "Köşe vuruşu\n",
      "Futbol kuralları\n",
      "Direkt serbest vuruş\n",
      "Ceza sahası\n",
      "Kale alanı\n",
      "Futbol sahası\n",
      "Türkiye Futbol Federasyonu\n",
      "Hasan Akıncıoğlu\n",
      "Türkiye Futbol Federasyonu\n",
      "Türkiye'deki futbol stadyumları listesi\n",
      "Karaelmas Kemal Köksal Stadyumu\n",
      "14 Eylül Stadyumu\n",
      "Diyarbakır Atatürk Stadyumu\n",
      "Diyarbakırspor\n",
      "Coşkun Demirbakan\n",
      "Yılmaz Vural\n",
      "Erdoğan Yılmaz\n",
      "Uğur Tütüneker\n",
      "Erkan Sözeri\n",
      "Hamza Hamzaoğlu\n",
      "2011-12 Süper Lig\n",
      "Hüseyin Avni Aker Stadyumu\n",
      "Şamil Ekinci Müzesi\n",
      "Sadri Şener Sosyal Tesisleri\n",
      "Trabzonspor teknik direktörleri listesi\n",
      "Kamuran Soykıray\n",
      "Mehmet Ali Çınar\n",
      "Hüsnü Özkara\n",
      "Eyüp Arın\n",
      "Türkiye Futbol Federasyonu\n",
      "Türkiye millî futbol takımı\n",
      "2008 Yaz Olimpiyatları'nda futbol\n",
      "1992 Yaz Olimpiyatları'nda futbol\n",
      "Uzatma süresi\n",
      "İskoçya Futbol Federasyonu\n",
      "Bulgaristan Futbol Birliği\n",
      "Cebelitarık Futbol Federasyonu\n",
      "Norveç Futbol Federasyonu\n",
      "Norveç\n",
      "İnternet üst seviye alan adları listesi\n",
      "Fransız Guyanası\n",
      "Fildişi Sahili\n",
      "Şeker\n",
      "Antik Çağ\n",
      "Antik Çağ'da savaş\n",
      "Orta Doğu\n",
      "Birleşik Krallık\n",
      "Kolombiya'nın departmanları\n",
      "Arauca (şehir)\n",
      "Kolombiya'nın idari bölümleri\n",
      "Kolombiya'nın departmanları\n",
      "Arauca (şehir)\n",
      "Kolombiya'nın idari bölümleri\n",
      "Üniter devlet\n",
      "Türkiye\n",
      "Kırgızistan\n",
      "Sovyetler Birliği\n",
      "Sovyetler Birliği Komünist Partisi\n",
      "Bolşevik Parti Marşı\n",
      "Nesokruşimaya i legendarnaya\n",
      "Sovyetler Birliği\n",
      "Doğu Almanya\n",
      "Tuva Halk Cumhuriyeti\n",
      "Suriye Selçukluları\n",
      "Hamitoğulları Beyliği\n",
      "Taşanoğulları Beyliği\n",
      "Altın Orda Devleti\n",
      "Dulkadiroğulları Beyliği\n",
      "Uygur Kağanlığı\n",
      "Oğuz Yabguluğu\n",
      "Oğuzlar\n",
      "Yıldız Han (mitoloji)\n",
      "Yıva boyu\n",
      "Üçoklar\n",
      "Yaparlı boyu\n",
      "Çavuldur boyu\n",
      "Türkmenistan\n",
      "Satın alma gücü paritesi\n",
      "Gelişme iktisadı\n",
      "Çalışma ekonomisi\n",
      "Ekonomik düşünce tarihi\n",
      "Davranışsal iktisat\n",
      "Ekonomik büyüme\n",
      "Döviz\n",
      "Oligopol piyasası\n",
      "Para politikası\n",
      "Fırsat maliyeti\n",
      "\n",
      "Completion: %0.20\n",
      "Time: 1.72 minutes\n",
      "\n",
      "İş döngüsü\n",
      "Arz şoku\n",
      "Rasyonel bekleyişler\n",
      "Satın alma gücü paritesi\n",
      "Pazar yapısı\n",
      "Davranışsal iktisat\n",
      "Merkez bankası\n",
      "Sermaye kaçışı\n",
      "Piyasa başarısızlığı\n",
      "Pareto verimliliği\n",
      "Pareto dağılımı\n",
      "Poisson dağılımı\n",
      "Poisson dağılımı tablosu\n",
      "Poisson dağılımı\n",
      "Ki-kare dağılımı\n",
      "Olasılık dağılımı\n",
      "Ki-kare dağılımı\n",
      "Beklenen değer\n",
      "Bağımsızlık\n",
      "Vatandaş\n",
      "İçişleri Bakanlığı\n",
      "Amerika Birleşik Devletleri İçişleri Bakanı\n",
      "Amerika Birleşik Devletleri\n",
      "Yunanistan Silahlı Kuvvetleri\n",
      "Bulgaristan Silahlı Kuvvetleri\n",
      "Danimarka Silahlı Kuvvetleri\n",
      "İtalya Silahlı Kuvvetleri\n",
      "Bosna-Hersek Silahlı Kuvvetleri\n",
      "Kuzey Makedonya Silahlı Kuvvetleri\n",
      "Çek Cumhuriyeti Silahlı Kuvvetleri\n",
      "Rusya Silahlı Kuvvetleri\n",
      "Bosna-Hersek Silahlı Kuvvetleri\n",
      "Çin\n",
      "Çin'de ulaşım\n",
      "Çin'de yolsuzluk\n",
      "Çin'deki üniversiteler listesi\n",
      "Beş Hanedan On Krallık\n",
      "Mançukuo\n",
      "Kyushu Açıkları Hava Muharebesi\n",
      "Uçak gemisi\n",
      "Admiral Kuznetsov (uçak gemisi)\n",
      "Hava üstünlüğü\n",
      "Müttefik Devletler\n",
      "Japonya'nın teslim oluşu\n",
      "Amerika Birleşik Devletleri\n",
      "İsviçre\n",
      "Schauspielhaus Zürich\n",
      "İsviçre\n",
      "Kurt Wüthrich\n",
      "ETH Zürih\n",
      "THE–QS Dünya Üniversite Sıralamaları\n",
      "Amerika Birleşik Devletleri\n",
      "Afganistan Savaşı (2001-günümüz)\n",
      "Amerika Birleşik Devletleri\n",
      "Amerikan Bağımsızlık Bildirisi\n",
      "Britanyalılar\n",
      "Şili\n",
      "Şili pesosu\n",
      "Şili arması\n",
      "Şili\n",
      "Fransa Millî Kütüphanesi\n",
      "Avustralya Millî Kütüphanesi\n",
      "Kütüphane\n",
      "Hristiyanlık\n",
      "İsa'nın çarmıha gerilişi\n",
      "İsa'nın başkalaşımı\n",
      "İsa'nın göğe yükselişi\n",
      "Hristiyanlık\n",
      "Doğu Ortodoks Kilisesi\n",
      "Kilise (örgüt)\n",
      "Karşı devrim\n",
      "9 Işık Doktrini\n",
      "Neofaşizm\n",
      "Prag Baharı\n",
      "Domuzlar Körfezi Çıkarması\n",
      "Kore'nin bölünmesi\n",
      "Amerika Birleşik Devletleri\n",
      "Amerika Birleşik Devletleri Senatosu\n",
      "Amerika Birleşik Devletleri Kongresi\n",
      "Bağımlı bölgeler ve ülkeler\n",
      "Büyük Okyanus\n",
      "Biskay Körfezi\n",
      "Çek Cumhuriyeti Millî Kütüphanesi\n",
      "Coğrafi koordinat sistemi\n",
      "Küre\n",
      "Matematiksel şekillerin listesi\n",
      "Çokgen\n",
      "Dörtgen\n",
      "Eşkenar dörtgen\n",
      "Öklid\n",
      "Yarıçap\n",
      "Çap\n",
      "Doğru\n",
      "Doğruluk (felsefe)\n",
      "Gerçek\n",
      "Felsefi gerçekçilik\n",
      "\n",
      "No new pages found! Resorting to known good pages.\n",
      "\n",
      "Ağaç\n",
      "Altınçanak\n",
      "İki çenekliler\n",
      "Bilimsel sınıflandırma\n",
      "\n",
      "Completion: %0.40\n",
      "Time: 1.37 minutes\n",
      "\n",
      "Biyomühendislik\n",
      "Doku kültürü\n",
      "Hücre kültürü\n",
      "Aşı (tıp)\n",
      "Cinsiyet steroidi\n",
      "Sayısal nesne tanımlayıcısı\n",
      "FDI Dünya Diş Hekimleri Birliği numaralandırması\n",
      "Mandibular birinci küçük azı dişi\n",
      "Maksiller orta kesici diş\n",
      "Mandibular orta kesici diş\n",
      "Mandibular yan kesici diş\n",
      "Köpek dişi\n",
      "Maksiller yan kesici diş\n",
      "Maksilla\n",
      "Parmak kemiği\n",
      "Kafatası\n",
      "Göbek\n",
      "Kıç\n",
      "Boşaltım sistemi\n",
      "Göz\n",
      "Bağışıklık sistemi\n",
      "Kongre Kütüphanesi Kontrol Numarası\n",
      "Kongre Kütüphanesi\n",
      "1812 Savaşı\n",
      "Amerika Birleşik Devletleri Donanması\n",
      "İkinci Berberi Savaşı\n",
      "Müslümanların Mağrip'i fethi\n",
      "Kıbrıs\n",
      "İsmet İnönü\n",
      "Ankara'da 1943 Türkiye genel seçimleri\n",
      "Bursa'da 1943 Türkiye genel seçimleri\n",
      "Tekirdağ'da 1943 Türkiye genel seçimleri\n",
      "Burdur'da 1943 Türkiye genel seçimleri\n",
      "1943 Türkiye genel seçimleri\n",
      "Çoruh'ta 1943 Türkiye genel seçimleri\n",
      "Artvin'de 1973 Türkiye genel seçimleri\n",
      "Türkiye Birlik Partisi\n",
      "1977 Türkiye genel seçimleri\n",
      "2011 Türkiye genel seçimleri\n",
      "Adana'da 2011 Türkiye genel seçimleri\n",
      "Milliyetçi Hareket Partisi\n",
      "Devrimci Sosyalist İşçi Partisi\n",
      "Demokrasi Zamanı Partisi\n",
      "Barış ve Eşitlik Partisi\n",
      "Toplumcu Kurtuluş Partisi (Türkiye)\n",
      "Çankaya\n",
      "Naci Çakır, Çankaya\n",
      "Barbaros, Çankaya\n",
      "Oğuzlar, Çankaya\n",
      "Çukurambar\n",
      "Şehit Cevdet Özdemir, Çankaya\n",
      "Koru, Çankaya\n",
      "Yukarı Bahçelievler, Çankaya\n",
      "Küçükesat, Çankaya\n",
      "Yukarı Bahçelievler, Çankaya\n",
      "Mimar Sinan, Çankaya\n",
      "Şehit Cengiz Karaca, Çankaya\n",
      "Beytepe, Çankaya\n",
      "Yayla, Çankaya\n",
      "Huzur, Çankaya\n",
      "Bahçelievler, Çankaya\n",
      "Üniversiteler, Çankaya\n",
      "Aşağı İmrahor, Çankaya\n",
      "Kavaklıdere, Çankaya\n",
      "Mustafa Kemal, Çankaya\n",
      "Ön Cebeci, Çankaya\n",
      "İç Anadolu Bölgesi\n",
      "Alkumru Barajı ve Hidroelektrik Santrali\n",
      "Tayfur Barajı\n",
      "Pusat-Özen Barajı\n",
      "Çatören Barajı\n",
      "Apa Barajı\n",
      "Düzağaç Akdeğirmen Barajı\n",
      "Kozağacı Barajı\n",
      "Çamlıgöze Barajı ve Hidroelektrik Santrali\n",
      "Kirazlıköprü Barajı ve Hidroelektrik Santrali\n",
      "Kürtün Barajı ve Hidroelektrik Santrali\n",
      "Dilimli Barajı\n",
      "Apa Barajı\n",
      "Deriner Barajı ve Hidroelektrik Santrali\n",
      "Umurbey Barajı\n",
      "Suat Uğurlu Barajı ve Hidroelektrik Santrali\n",
      "Berke Barajı ve Hidroelektrik Santrali\n",
      "Güç (elektrik)\n",
      "Joule yasası\n",
      "İç enerji\n",
      "İş\n",
      "İş (fizik)\n",
      "Uluslararası Standart Kitap Numarası\n",
      "Uluslararası Standart Müzik Numarası\n",
      "Elektronik veri değişimi\n",
      "Birleşmiş Milletler\n",
      "Birleşmiş Milletler Kalkınma Programı\n",
      "Türkiye\n",
      "Uluslararası Standart Kitap Numarası\n",
      "Tümleşik Otorite Dosyası\n",
      "Creative Commons lisansı\n",
      "Debian Özgür Yazılım Yönergeleri\n",
      "Gelişmiş Paketleme Aracı\n",
      "Paket yönetim sistemi\n",
      "\n",
      "Completion: %0.60\n",
      "Time: 1.34 minutes\n",
      "\n",
      "Gelişmiş Paketleme Aracı\n",
      "Portage (yazılım)\n",
      "PiSi Paket Yöneticisi\n",
      "YALI (yazılım)\n",
      "Yazılım lisansı\n",
      "Yazılım\n",
      "Çekirdek (bilgisayar bilimi)\n",
      "İş parçacığı\n",
      "İşletim sistemleri listesi\n",
      "Çoklu işleme\n",
      "Otomatikleştirilmiş muhakeme\n",
      "Eşzamanlılık denetimi\n",
      "Bilişsel bilim\n",
      "Model canlı\n",
      "Popülasyon genetiği\n",
      "Mayoz\n",
      "Hücre bölünmesi\n",
      "Hücre çekirdeği\n",
      "Çekirdekçik\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "\n",
      "Completion: %0.80\n",
      "Time: 1.45 minutes\n",
      "\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n",
      "Nükleozom\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-071f3a8b2110>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mgood_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraph_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrapeWikiArticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# scrape and make a query\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-21bcd65d9613>\u001b[0m in \u001b[0;36mscrapeWikiArticle\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# OPEN URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"firstHeading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-21bcd65d9613>\u001b[0m in \u001b[0;36mopen_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mopen_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                 resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[1;31m# Make the request on the httplib connection object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             httplib_response = self._make_request(\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1008\u001b[0m         \u001b[1;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[0;32m    412\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0m\u001b[0;32m    429\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         )\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\site-packages\\urllib3\\util\\ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1038\u001b[0m                         \u001b[1;31m# non-blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\oguza\\anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iter = 50000\n",
    "\n",
    "#url = \"https://tr.wikipedia.org/wiki/%C4%B0mroz_Deniz_Muharebesi_(1918)\"\n",
    "#url = \"https://tr.wikipedia.org/wiki/A%C4%9Fa%C3%A7\"\n",
    "url = \"https://tr.wikipedia.org/wiki/Puma_(%C5%9Firket)\"\n",
    "\n",
    "good_starts = [\"https://tr.wikipedia.org/wiki/A%C4%9Fa%C3%A7\",\n",
    "               \"https://tr.wikipedia.org/wiki/%C4%B0mroz_Deniz_Muharebesi_(1918)\",\n",
    "              \"https://tr.wikipedia.org/wiki/Karlovo,_Bulgaristan\",\n",
    "              \"https://tr.wikipedia.org/wiki/Akdamar,_Ak%C3%A7aabat\",\n",
    "              \"https://tr.wikipedia.org/wiki/Kahverengi_sa%C3%A7\",\n",
    "              \"https://tr.wikipedia.org/wiki/Osmanl%C4%B1_Devleti%27nde_%C4%B0slamc%C4%B1l%C4%B1k\",\n",
    "              \"https://tr.wikipedia.org/wiki/T%C3%BCm%C3%B6r\",\n",
    "              \"https://tr.wikipedia.org/wiki/Hipokrat\"]\n",
    "\n",
    "good_start_idx = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "title_list = []\n",
    "text_list = []\n",
    "counter = 0\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "    good_list, paragraph_texts, title = scrapeWikiArticle(url) # scrape and make a query\n",
    "\n",
    "    if title not in title_list:\n",
    "        title_list.append(title)\n",
    "        \n",
    "        for p in paragraph_texts: # update text list if title not included yet\n",
    "            text_list.append(p)\n",
    "\n",
    "    if good_list: # If there is a page returned, look for new pages\n",
    "\n",
    "        flag = False\n",
    "\n",
    "        for link in good_list:\n",
    "\n",
    "            try:\n",
    "                url_try = \"https://tr.wikipedia.org\" + link['href'] # try to open the link\n",
    "                soup_try = open_url(url_try)\n",
    "                head_try = soup_try.find(id=\"firstHeading\")\n",
    "                if head_try and (\".\" not in head_try.text) and (\":\" not in head_try.text):\n",
    "                    flag=True\n",
    "                    break\n",
    "\n",
    "            except:\n",
    "                print(\"A page failed!\")\n",
    "\n",
    "\n",
    "        if flag: # if a good page found update url and start over\n",
    "            \n",
    "            url = url_try # update url\n",
    "            \n",
    "        else:# if all k websites fail, go back and search larger \n",
    "            \n",
    "            new_soup = open_url(url)\n",
    "            good_list = find_turkish_wikis(new_soup,9) # find k turkish pages\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(\"\\nNo new pages found! Resorting to known good pages.\\n\")\n",
    "        url = good_starts[good_start_idx]\n",
    "        good_start_idx += 1\n",
    "        \n",
    "            \n",
    "    if (counter % 100) ==0 :\n",
    "        print(\"\\nCompletion: %{:.2f}\".format(100*counter/max_iter))\n",
    "        print(\"Time: {:.2f} minutes\\n\".format( (time.time()-start_time)/60 ))\n",
    "        start_time = time.time()\n",
    "           \n",
    "    \n",
    "print(\"\\n{} paragraphs are returned!\".format(len(text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fewer-chicken",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3641"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "criminal-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "\n",
    "    text = re.sub(r'\\[\\[[^]]*\\]\\]', '', text) # Get rid of [[ ]]\n",
    "    text = re.sub(r'\\[[^]]*\\]', '', text)   #   Or [ ]\n",
    "\n",
    "    text = re.sub(r'\\{\\{[^}]*\\}\\}', '', text)\n",
    "    text = re.sub(r'\\{[^}]*\\}', '', text)\n",
    "\n",
    "    text = re.sub(r'\\(\\([^)]*\\)\\)', '', text)\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    text = re.sub(r'\\=\\=([^\\=]*)\\=\\=', '', text)#   == ==\n",
    "    text = re.sub(r'\\<!--[^>]*\\-->', '', text)# <!-- -->\n",
    "    text = re.sub(r'(?m)^\\*.*\\n?', '', text)# *\n",
    "    text = re.sub(r'(?m)^\\=.*\\n?', '', text)# =\n",
    "    text = re.sub(r'(?m)^#.*\\n?', '', text) # Lines with #\n",
    "    text = re.sub(r'(?m)^\\|.*\\n?', '', text) # Lines with or symbol\n",
    "    text = re.sub(r'(?m)^\\s\\|.*\\n?', '', text)  # Lines with or symbol after whitespace\n",
    "\n",
    "    text = re.sub(r'(?m)^from:.*\\n?', '', text) #Lines starting with from:\n",
    "    text = re.sub(r'(?m)^Resim:.*\\n?', '', text)\n",
    "    text = re.sub(r'(?m)^Dosya:.*\\n?', '', text)\n",
    "    text = re.sub(r'(?m)^!.*\\n?', '', text)\n",
    "    text = re.sub(r'(?m)^::.*\\n?', '', text)\n",
    "    text = re.sub(r'(?m)^;.*\\n?', '', text)\n",
    "\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    text = re.sub(r'\\n\\n\\n', '', text)\n",
    "\n",
    "\n",
    "    #text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-detail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11257\n"
     ]
    }
   ],
   "source": [
    "wiki_cleaned = [clean_text(text) for text in text_list if '\\xa0' not in text]\n",
    "print(len(wiki_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reliable-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Wiki_sentences2\"\n",
    "\n",
    "with open(file_name+\".txt\", 'w',encoding=\"utf16\") as f:\n",
    "    \n",
    "    for sentence in wiki_cleaned:\n",
    "        f.write(sentence+'\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "\n",
    "#for run in [one_run,second_run, third_run]:\n",
    "for run in [aaa]:\n",
    "    for p in run:\n",
    "\n",
    "        m = re.search('\\s+.\\.\\s',p) #find I. II. III. stuff\n",
    "        if m:    \n",
    "            end = m.end()-2\n",
    "            p = x[:end] + x[end + 1:]\n",
    "\n",
    "        m = re.search('[0-9]+\\.+[0-9]+',p) #delete all numbers\n",
    "        if m:\n",
    "            start = m.start()\n",
    "            end = m.end()\n",
    "            #print(p[start:end])\n",
    "            p = x[:start] + x[end + 1:]\n",
    "                        \n",
    "\n",
    "        m = re.search('\\[.*?\\]',p) # Delete references\n",
    "        if m:\n",
    "            start = m.start()\n",
    "            end = m.end()\n",
    "            p = x[:start] + x[end + 1:]\n",
    "            \n",
    "        sentences = p.split(\". \")\n",
    "\n",
    "        for sentence in sentences: \n",
    "            if sentence: # check empty string\n",
    "            \n",
    "                if sentence[-1] != \".\": # enforce . in the end\n",
    "                    sentence += \".\"\n",
    "                    \n",
    "                    all_sentences.append(sentence)\n",
    "                    \n",
    "print(\"There are {} sentences!\".format(len(all_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for p in one_run:\n",
    "    counter += len(p)\n",
    "    \n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-marketing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-palmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "divine-girlfriend",
   "metadata": {},
   "source": [
    "text_list = []\n",
    "\n",
    "#url = \"https://tr.wikipedia.org/wiki/%C4%B0mroz_Deniz_Muharebesi_(1918)\"\n",
    "#url = \"https://tr.wikipedia.org/wiki/A%C4%9Fa%C3%A7\"\n",
    "url = \"https://tr.wikipedia.org/wiki/Puma_(%C5%9Firket)\"\n",
    "\n",
    "for i in range(500):\n",
    "    \n",
    "    url, paragraphs_list = scrapeWikiArticle(url)\n",
    "    \n",
    "    for p in paragraphs_list:\n",
    "    \n",
    "        text_list.append(p)\n",
    "        \n",
    "print(\"\\n{} paragraphs are returned!\".format(len(text_list)))\n",
    "\n",
    "\n",
    "def scrapeWikiArticle(url): \n",
    "             \n",
    "    response = requests.get(url=url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title_obj = soup.find(id=\"firstHeading\")\n",
    "    \n",
    "    title = title_obj.text\n",
    "    if (\":\" in title) or ('.' in title):                     \n",
    "        print(\"\\nInvalid Title!!!!\\n{}\".format(url))\n",
    "        \n",
    "                \n",
    "    print(title)\n",
    "    #print(title.text.strip(\"\\n\"))\n",
    "    \n",
    "    paragraph_texts = get_paragraphs(soup) # get the paragraph from the wiki\n",
    "\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\") # find new wikis\n",
    "    random.shuffle(allLinks)\n",
    "\n",
    "    linkToScrape,flag = find_turkish_wiki(allLinks) # find a turkish one\n",
    "    \n",
    "    if not flag:\n",
    "        print(\"Search Failed!\")\n",
    "    \n",
    "    new_url = \"https://tr.wikipedia.org\" + linkToScrape['href']\n",
    "    \n",
    "    return new_url, paragraph_texts\n",
    "\n",
    "\n",
    "turkish_char = ['ç',\"Ç\",'ğ',\"Ğ\",'ı','ö',\"Ö\",'ş',\"Ş\",'ü',\"Ü\"] #,\"I\"\n",
    "\n",
    "def find_turkish_wiki(allLinks):\n",
    "    \n",
    "    flag = False # Control flag for the search\n",
    "    \n",
    "    turkish_link = allLinks[-1] #initialize with a random link\n",
    "    \n",
    "    for link in allLinks:\n",
    "        \n",
    "        if flag:\n",
    "            break\n",
    "        \n",
    "        try: \n",
    "            \n",
    "            if link['href'].find(\"/wiki/\") == -1: # find links to other wikis\n",
    "                continue\n",
    "                \n",
    "            link_title = link['title'] # find links that have a title\n",
    "            \n",
    "            \n",
    "            \"\"\"if title.split(\":\")[0] in [\"Kategori\",\"Kategoriler\",\"Vikipedi\",\"Dosya\",\n",
    "                                       \"Tartışma\",\"Anasayfa\",\"Özel\",\"Şablon\"]: #Skip such wikis\"\"\"\n",
    "            #(\".png\" in link_title) or (\".jpg\" in link_title) or (\".svg\" in link_title): #Skip such wikis\n",
    "            \n",
    "            if (\":\" in link_title) or ('.' in link_title):                     \n",
    "                continue \n",
    "                \n",
    "            for char in turkish_char: # check if the title includes a Turkish Character\n",
    "\n",
    "                if char in link_title:\n",
    "                    turkish_link = link\n",
    "                    flag = True\n",
    "                    break\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    return turkish_link,flag\n",
    "\n",
    "def get_paragraphs(soup):\n",
    "    \n",
    "    paragraphs = soup.find(id=\"bodyContent\").find_all('p') # find all paragraphs\n",
    "\n",
    "    paragraph_texts = []\n",
    "    for p in paragraphs:\n",
    "        cleaned_text = p.text.strip(\"\\n\")\n",
    "        if cleaned_text: # if the paragraph is not empty\n",
    "            paragraph_texts.append(cleaned_text)\n",
    "            \n",
    "    return paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tr.wikipedia.org/wiki/%C4%B0mroz_Deniz_Muharebesi_(1918)\"\n",
    "\n",
    "response = requests.get(url=url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "title = soup.find(id=\"firstHeading\")\n",
    "print(title.text)\n",
    "\n",
    "allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "random.shuffle(allLinks)\n",
    "linkToScrape = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWikiArticle(url,text_list=[],counter=0,max_count=10): \n",
    "    \n",
    "    if counter != max_count:\n",
    "            \n",
    "        response = requests.get(url=url)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title = soup.find(id=\"firstHeading\")\n",
    "        title_text = title.text.strip(\"\\n\")\n",
    "        print(title_text)\n",
    "\n",
    "        allLinks = soup.find(id=\"bodyContent\").find_all(\"a\")\n",
    "        random.shuffle(allLinks)\n",
    "        \n",
    "        linkToScrape = find_turkish_link(allLinks)\n",
    "        paragraph_texts = get_paragraphs(soup)\n",
    "        \n",
    "        text_list.append(paragraph_texts)    \n",
    "\n",
    "        counter += 1\n",
    "        scrapeWikiArticle(\"https://tr.wikipedia.org\" + linkToScrape['href'],counter,max_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
